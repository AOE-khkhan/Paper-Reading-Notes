# Terminology
This document contains important information on many topics listed below, related to my PhD project on `Application of Deep Learning for the Decomposition of 3D Shape for Hexahedral Mesh Generation`.
- [Machine Learning](Machine_Learning)
- [CAD/CAE](CAD/CAE)
- [Programming](Programming)

## Machine_Learning
### Context-Free Grammars

A *context-free grammar (CFG)* is a set of recursive writing rules (or **productions**) used to generate patterns of strings.

A CFG consists of the following components:
- A set of **terminal symbols**, which are the characters of the alphabet that appear in the strings generated by the grammar.
- A set of **nonterminal symbols**, which are placeholders for patterns of terminal symbols that can be generated by the nonterminal symbols.
- A set of **productions**, which are rules for replacing (or rewriting) nonterminal symbols (on the left side of the production) in a string with other nonterminal or terminal symbols (on the right side of the production).
- A **start symbol**, which is a special nonterminal symbol that appears in the intial string generated by the grammar.

To generate a string of terminal symbols from a CFG, we:
- Begin with a string consisting of the start symbol.
- Apply one of the productions with the start symbol on the left hand size, replacing the start symbol with the right hand sidde of the production.
- Repeat the process of selecting nonterminal symbols in the string,and replacing them with the right hand side of some corresponding production, until all nonterminals have been replaced by terminal symbols.

Source: https://www.cs.rochester.edu/~nelson/courses/csc_173/grammars/cfg.html

## CAD/CAE
- [Geometric Idealisation](Geometric_Idealisation)
- [Geometric Clean Up](Geometric_Clean_Up)
- [CAD/CAE Integration](CAD/CAE_Integration)

### Geometric_Idealisation

### Geometric_Clean_Up

### CAD/CAE_Integration

## Programming
- [Python](Python)
- [Pytorch](Pytorch)

### Python

<hr>

### Pytorch
Pytorch differs from Tensorflow and Caffe etc. due to the unique way it builds its neural networks. These other machine learning frameworks use a **static** method, where one has to build a neural network and reuse the same structure again and again. This means that if you want to change how the network behaves you must start from scratch again.

Pytorch instead using a technique called *reverse-mode auto differentiation*, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. This means that it has a **dynamic** method of building its graph of the network, where the graph is built on the fly. This allows for more flexiblity and is why Pytorch is becoming more dominant in research applications.



#### Functions
<details>
  <summary><span><code>view(<i>*shape</i>) -> Tensor</code></span><br/> <blockquote>Returns a tensor with the same data as the <b>self</b> tensor but of a different <b>shape</b>.<blockquote></summary>
<hr>
  <pre>
  <code>
  >>> x = torch.randn(4, 4)
  >>> x.size()
  <b>torch.Size([4, 4])</b>
  >>> y = x.view(16)
  >>> y.size()
  <b>torch.Size([16])</b>
  >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions<br>
  >>> z.size()
  <b>torch.Size([2, 8])</b>
  </code>
  </pre>
<hr>
</details>

